# REAL SESSION EXAMPLES

Documentation of actual Desktop + Claude Code coordination sessions, showing the methodology in practice.

---

## SESSION 1: Agent Coordinator System Development
**Date:** December 26, 2025  
**Project:** agent-coordinator  
**Objective:** Understand current state, design improvements, test actual execution  
**Duration:** ~3 hours  
**Checkpoints Completed:** 4

---

### Opening Message (to Desktop Claude)

```
I'm working on a multi-LLM orchestration system.

Context: I have a system that coordinates GLM, Claude, Codex, and Gemini agents. 
It's been developed but never actually run. File-based communication, cost-efficient 
(using CLI plans not APIs).

Current issues:
- Don't know how environment is actually configured
- No state management (no start/stop commands)
- Need deployment strategy for 10+ projects
- System has never been tested end-to-end

Create checkpoint-based prompts for Claude Code to:
1. Analyze environment configuration
2. Design state management system
3. Determine deployment model
4. Test actual agent invocation

Output to: \\100.106.201.33\cdc\Storage\projects\agent-coordinator\docs\coordination\

Start with Checkpoint 1: Environment Analysis
```

---

### CHECKPOINT 1: Environment Configuration Analysis

**Prompt generated by Desktop Claude:**

```markdown
# Environment Configuration Analysis

## CONTEXT
Multi-LLM system uses CLI plans (not APIs) for cost efficiency. 
Environment configuration unclear - need to document reality.

## TASK
Document current environment configuration.

## OUTPUT FORMAT
Create: \\100.106.201.33\cdc\Storage\projects\agent-coordinator\docs\coordination\CHECKPOINT_1_ENV.md

Include:
1. Current State (table of key config locations)
2. How It Works (sourcing order diagram)
3. What Needs to Change (issues list)
4. Recommendation (single sentence)

## CONSTRAINTS
- NO code dumps
- Use tables for clarity
- Focus on WHAT exists, not WHY

## SUCCESS CRITERIA
- [ ] Location of global config identified
- [ ] GLM version documented
- [ ] Key variables mapped
- [ ] Mismatches identified
```

**Results:**

Key findings from CHECKPOINT_1_ENV.md:
- Global config at `~/.config/secrets/ai.env`
- GLM accessed via Z.ai Anthropic-compatible endpoint (not direct API)
- `ANTHROPIC_BASE_URL="https://api.z.ai/api/anthropic"`
- Scripts had wrong endpoint and wrong env vars
- Hardcoded old versions (glm-4.5 vs glm-4.7)

**Decision point:**
✅ Approved - Use Z.ai endpoint pattern globally, update version references

---

### CHECKPOINT 2: State Management Design

**Prompt generated by Desktop Claude:**

```markdown
# State Management & Lifecycle Design

## CONTEXT
Environment analysis complete. Need proper lifecycle with start/stop commands.

## TASK
Design state management system with clear lifecycle.

## OUTPUT FORMAT
Create: \\100.106.201.33\cdc\Storage\projects\agent-coordinator\docs\coordination\CHECKPOINT_2_STATE.md

Include:
1. State diagram (mermaid)
2. State schema (JSON)
3. Startup sequence (numbered steps)
4. Stop sequence (numbered steps)
5. Environment checks list

## CONSTRAINTS
- NO implementation code (skeleton only)
- Use mermaid for diagrams
- Maximum 5 pages

## SUCCESS CRITERIA
- [ ] States defined (STOPPED, STARTING, ACTIVE, STOPPING)
- [ ] Transitions clear
- [ ] Startup checks environment
- [ ] Stop includes cleanup
```

**Results:**

CHECKPOINT_2_STATE.md delivered:
- Clean state machine with 4 states
- Startup verifies Z.ai endpoint, creates `.agents/` structure
- Stop waits for agents (30s timeout), runs garbage collection
- Update check on start (non-blocking)

**Decision point:**
Question raised: Should VERSION be external file or in code?
✅ Decision: External VERSION file (cleaner, standard practice)

---

### CHECKPOINT 3: Deployment Strategy

**Prompt generated by Desktop Claude:**

```markdown
# Dev vs Deploy Strategy

## CONTEXT
Charles has 10+ projects using agent-coordinator. Changes need to propagate properly.
Security critical: ~/.config/secrets/ai.env must NEVER be committed.

## TASK
Evaluate deployment options and recommend one.

## OUTPUT FORMAT
Create: \\100.106.201.33\cdc\Storage\projects\agent-coordinator\docs\coordination\CHECKPOINT_3_DEPLOY.md

Include:
1. Current state (honest assessment)
2. Option comparison (A: submodule, B: symlink, C: install script)
3. Recommended approach with justification
4. Migration steps
5. Security implementation (.gitignore, pre-commit hooks)

## CONSTRAINTS
- Evaluate ALL three options fairly
- Security is non-negotiable
- Must work for single-user/single-machine

## SUCCESS CRITERIA
- [ ] Current mess documented
- [ ] Clear recommendation made
- [ ] Migration plan provided
- [ ] Security measures defined
```

**Results:**

CHECKPOINT_3_DEPLOY.md revealed:
- Projects already using git submodules (old agent-system)
- Two separate repos exist (agent-system vs agent-coordinator)
- Recommended: Option B (Symlink) - best for single user, active development
- Security: .gitignore patterns, pre-commit hook to block secrets

**Decision point:**
Discovery: Git submodules already in place but stale
✅ Approved: Migrate to symlink model, remove old submodules

---

### CHECKPOINT 4: First Agent Invocation

**Prompt generated by Desktop Claude:**

```markdown
# Integration Test - Make The System Actually Work

## CONTEXT
System designed but never run. Need one successful agent invocation.

## TASK
Wire up and test actual agent execution.

## OUTPUT FORMAT
Create: \\100.106.201.33\cdc\Storage\projects\agent-coordinator\docs\coordination\CHECKPOINT_4_TEST.md

Include:
1. Test command executed
2. Actual results
3. What was fixed
4. Next blockers
5. File writing rules implementation
6. Garbage collector creation

## CONSTRAINTS
- Just get ONE agent working
- Don't try to fix everything
- Document what broke and how it was fixed

## SUCCESS CRITERIA
- [ ] GLM responds via coordinator
- [ ] Result returned successfully
- [ ] File writing rules defined
- [ ] Garbage collector script created
```

**Results:**

CHECKPOINT_4_TEST.md - SUCCESS:
- Fixed `glm_direct.py` to use Z.ai endpoint
- First successful GLM-4.7 invocation via coordinator
- Created `config/agent_rules.json` (block root writes)
- Created `scripts/garbage_collector.py`

**Critical fix:**
Changed from `GLM_API_KEY` + wrong endpoint to `ANTHROPIC_AUTH_TOKEN` + Z.ai endpoint

**Decision point:**
✅ System works! Ready for visibility implementation (Part C)

---

## SESSION ANALYSIS

### What Worked

1. **Bounded checkpoints** - Each had ONE clear goal
2. **Explicit file paths** - No confusion about output location
3. **Success criteria** - Clear definition of done
4. **Discovery-driven** - Each checkpoint informed the next
5. **Human decisions** - Strategic choices made at each gate

### What Prevented Hallucinations

1. **Desktop created prompts** - CC couldn't invent requirements
2. **File-based outputs** - Tangible, reviewable results
3. **Checkpoint gates** - Work stopped for review
4. **Explicit constraints** - "NO code dumps", "use diagrams"
5. **Evaluation cycle** - Desktop reviewed every output

### Progression Pattern

```
Checkpoint 1 (Analysis)
    ↓ Discovered reality ≠ assumptions
Checkpoint 2 (Design)
    ↓ Designed based on reality
Checkpoint 3 (Strategy)
    ↓ Chose deployment model
Checkpoint 4 (Test)
    ✓ Validated everything works
```

Each checkpoint built on previous discoveries. Couldn't have designed state management (2) without understanding environment (1). Couldn't choose deployment (3) without state design (2). Couldn't test (4) without all the prior context.

---

## KEY MOMENTS

### Moment 1: The Z.ai Discovery (Checkpoint 1)

**What happened:**
Checkpoint 1 revealed Charles was using Z.ai's Anthropic-compatible endpoint to access GLM, not direct GLM API.

**Why it mattered:**
This was the core cost-saving hack - using subscription plans via CLI instead of APIs. The entire system architecture depended on understanding this pattern.

**How checkpoints helped:**
If we'd jumped straight to testing (Checkpoint 4), we would have failed without understanding the Z.ai pattern. The analysis checkpoint (1) uncovered the critical detail.

---

### Moment 2: VERSION File Decision (Checkpoint 2)

**What happened:**
During state management design, question arose: VERSION file external or embedded in code?

**Desktop's response:**
```
Keeping it external is correct for these reasons:
1. ✅ Single source of truth - one file to update
2. ✅ No code changes for version bumps
3. ✅ Git-friendly - easy to see version history
4. ✅ Update checks work - can read file without importing code
5. ✅ Standard practice - Python packages do this
```

**Why it mattered:**
Small decision, but making it explicitly prevented later confusion. Documented reasoning means anyone can understand WHY this choice was made.

---

### Moment 3: Deployment Model Choice (Checkpoint 3)

**What happened:**
Three options presented (submodule, symlink, install script). Had to choose ONE.

**Analysis:**
| Option | Update Friction | Version Control | Breaking Changes |
|--------|----------------|-----------------|------------------|
| Submodule | Medium | Excellent | Safe |
| Symlink | Excellent | Poor | Risky |
| Install Script | High | Excellent | Safe |

**Decision:**
Option B (Symlink) - justified by: single user, active development, same machine, easy rollback.

**Why checkpoints helped:**
Forced explicit comparison before deciding. Documented trade-offs. Can revisit if situation changes (e.g., multiple developers).

---

### Moment 4: First Successful Run (Checkpoint 4)

**What happened:**
System that had "never been run" successfully invoked GLM-4.7 and got response.

**The fix:**
```python
# Wrong (what was in code):
API_KEY = os.environ.get('GLM_API_KEY')
BASE_URL = 'https://api.z.ai/api/coding/paas/v4'

# Right (after understanding from Checkpoint 1):
API_KEY = os.environ.get('ANTHROPIC_AUTH_TOKEN')
BASE_URL = os.environ.get('ANTHROPIC_BASE_URL')  # points to Z.ai
```

**Why checkpoints helped:**
Couldn't have fixed this without Checkpoint 1's environment analysis. The discovery → design → implement → test flow prevented wasted effort.

---

## LESSONS LEARNED

### 1. Analysis Before Action

Starting with Checkpoint 1 (environment analysis) instead of jumping to implementation prevented:
- Wasted effort fixing wrong things
- Making assumptions that didn't match reality
- Breaking working configurations

**Lesson:** Always start with "what exists?" before "what should we build?"

---

### 2. One Thing at a Time

Each checkpoint had 3-5 deliverables, not 20. This:
- Kept CC focused
- Made review manageable
- Allowed course correction
- Prevented overwhelm

**Lesson:** Resist urge to "do it all at once"

---

### 3. File Paths Are Critical

Every checkpoint specified exact file path:
```
\\100.106.201.33\cdc\Storage\projects\agent-coordinator\docs\coordination\CHECKPOINT_N_*.md
```

This prevented:
- Files in project root
- Lost documentation
- "Where did that go?" moments

**Lesson:** Explicit > implicit, always

---

### 4. Success Criteria Define Done

Each checkpoint had checkbox criteria:
```
## SUCCESS CRITERIA
- [ ] States defined (STOPPED, STARTING, ACTIVE, STOPPING)
- [ ] Transitions clear
- [ ] Startup checks environment
```

This made evaluation objective, not subjective.

**Lesson:** Define "done" before starting

---

### 5. Discovery Changes Plans

Original plan vs actual:
- **Plan:** Fix GLM API calls
- **Reality:** Not using GLM API at all, using Z.ai endpoint

Checkpoint structure allowed pivoting when reality ≠ assumptions.

**Lesson:** Stay flexible, follow the evidence

---

## ANTI-EXAMPLES (What NOT to Do)

### ❌ Bad: Combined Checkpoint

```markdown
# CHECKPOINT 1: Complete System Overhaul

Analyze environment, design state management, choose deployment, 
implement everything, test it all, write documentation.

Output: Everything to various locations
Success criteria: System works
```

**Why this fails:**
- Too broad
- No clear evaluation point
- Can't course-correct
- Overwhelms CC
- Produces low-quality results

---

### ❌ Bad: Vague Objective

```markdown
# CHECKPOINT 2: Improve Things

Make the system better.

Output: Somewhere
Success criteria: It's improved
```

**Why this fails:**
- CC will hallucinate requirements
- No way to evaluate success
- No clear deliverable
- File location ambiguous

---

### ❌ Bad: No Constraints

```markdown
# CHECKPOINT 3: Design Architecture

Create system design.

[No constraints section]
[No file path]
[No success criteria]
```

**Why this fails:**
- CC might produce 100-page document
- Could include full implementation code
- Unknown output location
- Can't tell if it's done

---

## METRICS FROM THIS SESSION

**Time investment:**
- 4 checkpoints in ~3 hours
- Average ~45 min per checkpoint
- Includes review and decision time

**Output quality:**
- Zero hallucinated features
- All files in correct locations
- Every checkpoint useful
- Nothing had to be redone

**Knowledge transfer:**
- System fully documented
- Can resume from any checkpoint
- New contributor could understand flow
- Decisions have recorded rationale

**Business value:**
- System went from "never run" to "working"
- Deployment strategy chosen
- Security measures defined
- Ready for next phase (visibility)

---

## WHAT'S NEXT (Based on TODO.md)

**High priority from this session:**
1. Visibility system (agent monitoring dashboard)
2. Safe self-development (sandbox for testing changes)
3. Task tracking integration (system maintains TODO)

**Why checkpoints enable this:**
Each of these can be broken into 2-3 checkpoints:

```
Visibility: 
  → Checkpoint 5: Design monitoring display
  → Checkpoint 6: Implement status writers
  → Checkpoint 7: Test real-time updates

Self-development:
  → Checkpoint 8: Sandbox strategy
  → Checkpoint 9: Merge/rollback procedure
  → Checkpoint 10: Test with actual changes
```

The pattern continues...

---

## CONCLUSION

This session demonstrated that Desktop + Claude Code coordination:
- ✅ Prevents hallucinations (bounded prompts)
- ✅ Maintains focus (one checkpoint at a time)
- ✅ Enables course correction (review between checkpoints)
- ✅ Produces quality outputs (clear criteria)
- ✅ Documents decisions (rationale captured)
- ✅ Allows resumption (TODO.md + checkpoints)

The methodology is proven effective for complex, multi-phase projects where traditional "one prompt gets everything" approaches fail.
